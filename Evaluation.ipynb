{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d165dc-571c-476e-97af-d0bfaed1e431",
   "metadata": {},
   "source": [
    "# Evaluation of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1419a0c-34bf-4e17-b046-3fe52a5fd12d",
   "metadata": {},
   "source": [
    "### Task breakdown:\n",
    "\n",
    "#### 1. Load Benchmark data for each query.\n",
    "\n",
    "#### 2. Load Ranked Documents for each query.\n",
    "\n",
    "#### 3. Evaluate the data\n",
    "\n",
    "#### 4. Compare the models from the results obtained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfc7c6-43a9-4b5b-9a54-f0e48cdaa75f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing code for evaluating model for 1 query with 1 model's result for that query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ca2d6-e963-4d8b-9566-722f62f594d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "For Query R101 with benchmark 'Dataset101' and result with BM25 Model for Query 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "938cbf31-79d5-4cf7-ace6-7e26f33d09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_benchmark_101_file_path = 'EvaluationBenchmark/Dataset101.txt'\n",
    "bm25model_r101_result ='My Code/Outputs-Task1-New/BM25_R101Ranking.dat' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4237c8a-fee9-40d8-861e-e4b540225546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Benchmark Data:\n",
      "{'6146': 0.0, '18586': 0.0, '22170': 0.0, '22513': 0.0, '26642': 0.0, '26847': 0.0, '27577': 0.0, '30647': 0.0, '39496': 1.0, '46547': 1.0, '46974': 1.0, '61329': 0.0, '61780': 0.0, '62325': 1.0, '63261': 1.0, '77909': 0.0, '80425': 0.0, '80950': 0.0, '81463': 0.0, '82330': 1.0, '82454': 1.0, '82912': 0.0, '83167': 0.0}\n",
      "Loaded Ranked Documents:\n",
      "{'1': '61780', '2': '63261', '3': '46974', '4': '46547', '5': '22170', '6': '62325', '7': '6146', '8': '22513', '9': '61329', '10': '39496', '11': '77909', '12': '82330', '13': '18586', '14': '80950', '15': '83167', '16': '82454', '17': '27577', '18': '26847', '19': '30647', '20': '80425', '21': '26642', '22': '81463', '23': '82912'}\n",
      "For task 2:\n",
      "Evaluating Document ID: 61780 at Rank: 1\n",
      "Document ID: 61780 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 63261 at Rank: 2\n",
      "At position 2, precision= 0.5, recall= 0.14285714285714285\n",
      "Evaluating Document ID: 46974 at Rank: 3\n",
      "At position 3, precision= 0.6666666666666666, recall= 0.2857142857142857\n",
      "Evaluating Document ID: 46547 at Rank: 4\n",
      "At position 4, precision= 0.75, recall= 0.42857142857142855\n",
      "Evaluating Document ID: 22170 at Rank: 5\n",
      "Document ID: 22170 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 62325 at Rank: 6\n",
      "At position 6, precision= 0.6666666666666666, recall= 0.5714285714285714\n",
      "Evaluating Document ID: 6146 at Rank: 7\n",
      "Document ID: 6146 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 22513 at Rank: 8\n",
      "Document ID: 22513 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 61329 at Rank: 9\n",
      "Document ID: 61329 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 39496 at Rank: 10\n",
      "At position 10, precision= 0.5, recall= 0.7142857142857143\n",
      "Evaluating Document ID: 77909 at Rank: 11\n",
      "Document ID: 77909 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 82330 at Rank: 12\n",
      "At position 12, precision= 0.5, recall= 0.8571428571428571\n",
      "Evaluating Document ID: 18586 at Rank: 13\n",
      "Document ID: 18586 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 80950 at Rank: 14\n",
      "Document ID: 80950 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 83167 at Rank: 15\n",
      "Document ID: 83167 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 82454 at Rank: 16\n",
      "At position 16, precision= 0.4375, recall= 1.0\n",
      "Evaluating Document ID: 27577 at Rank: 17\n",
      "Document ID: 27577 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 26847 at Rank: 18\n",
      "Document ID: 26847 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 30647 at Rank: 19\n",
      "Document ID: 30647 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 80425 at Rank: 20\n",
      "Document ID: 80425 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 26642 at Rank: 21\n",
      "Document ID: 26642 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 81463 at Rank: 22\n",
      "Document ID: 81463 is found in benchmark but not relevant.\n",
      "Evaluating Document ID: 82912 at Rank: 23\n",
      "Document ID: 82912 is found in benchmark but not relevant.\n",
      "---The average precision = 0.5744047619047619\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import sys\n",
    "    import os\n",
    "    #import coll\n",
    "    #import df\n",
    "\n",
    "\n",
    "    # task 2 evaluation\n",
    "    # get the benchmark\n",
    "    benFile = open(query_benchmark_101_file_path)\n",
    "    #benFile = open('Training_benchmark.txt')\n",
    "    file_ = benFile.readlines()\n",
    "    ben={}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]]=float(lineList[2])\n",
    "    benFile.close()\n",
    "    \n",
    "    # Print the loaded benchmark data\n",
    "    print(\"Loaded Benchmark Data:\")\n",
    "    print(ben)\n",
    "    \n",
    "    # number documents \n",
    "    rank1 = {}\n",
    "    i = 1\n",
    "    for line in open(bm25model_r101_result):\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        # Strip '.xml' extension from document IDs\n",
    "        doc_id = lineList[0].replace('.xml', '')\n",
    "        rank1[str(i)] = doc_id\n",
    "        i += 1\n",
    "        \n",
    "    #Print the loaded ranked documents\n",
    "    print(\"Loaded Ranked Documents:\")\n",
    "    print(rank1)\n",
    "\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"For task 2:\")\n",
    "    ri = 0\n",
    "    map1 = 0.0\n",
    "    R = len([id for (id, v) in ben.items() if v > 0])\n",
    "    if R == 0:\n",
    "        print(\"No relevant documents in the benchmark.\")\n",
    "    else:\n",
    "        for (n, doc_id) in sorted(rank1.items(), key=lambda x: int(x[0])):\n",
    "            print(f\"Evaluating Document ID: {doc_id} at Rank: {n}\")\n",
    "            if doc_id in ben:\n",
    "                if ben[doc_id] > 0:\n",
    "                    ri += 1\n",
    "                    pi = float(ri) / float(int(n))\n",
    "                    recall = float(ri) / float(R)\n",
    "                    map1 += pi\n",
    "                    print(f\"At position {int(n)}, precision= {pi}, recall= {recall}\")\n",
    "                else:\n",
    "                    print(f\"Document ID: {doc_id} is found in benchmark but not relevant.\")\n",
    "            else:\n",
    "                # Instead of printing not found in benchmark for each document, count the not found documents\n",
    "                print(f\"Document ID: {doc_id} is not found in benchmark.\")\n",
    "        if ri > 0:\n",
    "            map1 = map1 / float(ri)\n",
    "        else:\n",
    "            map1 = 0\n",
    "        print(\"---The average precision = \" + str(map1))\n",
    "\n",
    "        if ri == 0:\n",
    "            print(\"No relevant documents found in the ranked results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29652e-6f06-40b4-9b56-8514a23a1270",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now Testing code for evaluating model for all the queries benchmark with BM25 model's results for those querries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3100cf0-9bc1-4013-8f60-1e174ec3a8bb",
   "metadata": {},
   "source": [
    "For queries 101 to 150 with benchmarks 101...150 for BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1a1a876-ecdf-431a-997a-1a1bb3a371c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Query 101: relevant documents (ri)=7, recall=1.0, MAP=0.5744047619047619\n",
      "For Query 102: relevant documents (ri)=135, recall=1.0, MAP=0.6363322454881951\n",
      "For Query 103: relevant documents (ri)=14, recall=1.0, MAP=0.22607885048707604\n",
      "For Query 104: relevant documents (ri)=120, recall=1.0, MAP=0.4822634740278421\n",
      "For Query 105: relevant documents (ri)=16, recall=1.0, MAP=0.4478223326042083\n",
      "For Query 106: relevant documents (ri)=4, recall=1.0, MAP=0.43599791013584116\n",
      "For Query 107: relevant documents (ri)=3, recall=1.0, MAP=0.53125\n",
      "For Query 108: relevant documents (ri)=3, recall=1.0, MAP=0.08409961685823757\n",
      "For Query 109: relevant documents (ri)=20, recall=1.0, MAP=0.3985829218274427\n",
      "For Query 110: relevant documents (ri)=5, recall=1.0, MAP=0.26757964257964256\n",
      "For Query 111: relevant documents (ri)=3, recall=1.0, MAP=0.11858076563958915\n",
      "For Query 112: relevant documents (ri)=6, recall=1.0, MAP=0.4932178932178932\n",
      "For Query 113: relevant documents (ri)=12, recall=1.0, MAP=0.34900356664664495\n",
      "For Query 114: relevant documents (ri)=5, recall=1.0, MAP=0.7588235294117647\n",
      "For Query 115: relevant documents (ri)=3, recall=1.0, MAP=0.18204365079365079\n",
      "For Query 116: relevant documents (ri)=16, recall=1.0, MAP=0.39018475400608155\n",
      "For Query 117: relevant documents (ri)=3, recall=1.0, MAP=0.31666666666666665\n",
      "For Query 118: relevant documents (ri)=3, recall=1.0, MAP=0.4666666666666666\n",
      "For Query 119: relevant documents (ri)=4, recall=1.0, MAP=0.15200320512820512\n",
      "For Query 120: relevant documents (ri)=9, recall=1.0, MAP=0.24999157396320518\n",
      "For Query 121: relevant documents (ri)=14, recall=1.0, MAP=0.3125488389708701\n",
      "For Query 122: relevant documents (ri)=15, recall=1.0, MAP=0.5245554371668922\n",
      "For Query 123: relevant documents (ri)=3, recall=1.0, MAP=0.14183006535947715\n",
      "For Query 124: relevant documents (ri)=6, recall=1.0, MAP=0.14701930014430012\n",
      "For Query 125: relevant documents (ri)=12, recall=1.0, MAP=0.6080335639634574\n",
      "For Query 126: relevant documents (ri)=19, recall=1.0, MAP=0.871187113502701\n",
      "For Query 127: relevant documents (ri)=5, recall=1.0, MAP=0.3933333333333333\n",
      "For Query 128: relevant documents (ri)=4, recall=1.0, MAP=0.14345238095238094\n",
      "For Query 129: relevant documents (ri)=17, recall=1.0, MAP=0.23149351723154854\n",
      "For Query 130: relevant documents (ri)=3, recall=1.0, MAP=0.8055555555555555\n",
      "For Query 131: relevant documents (ri)=4, recall=1.0, MAP=0.21388888888888888\n",
      "For Query 132: relevant documents (ri)=7, recall=1.0, MAP=0.5611560008618832\n",
      "For Query 133: relevant documents (ri)=5, recall=1.0, MAP=0.22824508824508824\n",
      "For Query 134: relevant documents (ri)=5, recall=1.0, MAP=0.242005772005772\n",
      "For Query 135: relevant documents (ri)=14, recall=1.0, MAP=0.45176637481949194\n",
      "For Query 136: relevant documents (ri)=8, recall=1.0, MAP=0.21572420634920636\n",
      "For Query 137: relevant documents (ri)=3, recall=1.0, MAP=0.3833333333333333\n",
      "For Query 138: relevant documents (ri)=7, recall=1.0, MAP=0.08467770645300712\n",
      "For Query 139: relevant documents (ri)=3, recall=1.0, MAP=0.2686868686868687\n",
      "For Query 140: relevant documents (ri)=11, recall=1.0, MAP=0.7456621215979505\n",
      "For Query 141: relevant documents (ri)=24, recall=1.0, MAP=0.3697963021092741\n",
      "For Query 142: relevant documents (ri)=4, recall=1.0, MAP=0.15625\n",
      "For Query 143: relevant documents (ri)=4, recall=1.0, MAP=0.10972222222222223\n",
      "For Query 144: relevant documents (ri)=6, recall=1.0, MAP=0.1530862815345574\n",
      "For Query 145: relevant documents (ri)=5, recall=1.0, MAP=0.3502081913846619\n",
      "For Query 146: relevant documents (ri)=13, recall=1.0, MAP=0.3079157719709639\n",
      "For Query 147: relevant documents (ri)=6, recall=1.0, MAP=0.15392209234121\n",
      "For Query 148: relevant documents (ri)=12, recall=1.0, MAP=0.2998325543446157\n",
      "For Query 149: relevant documents (ri)=5, recall=1.0, MAP=0.21543917435221785\n",
      "For Query 150: relevant documents (ri)=4, recall=1.0, MAP=0.25952445899254406\n",
      "\n",
      "Overall results for 50 queries:\n",
      "Total relevant items found: 639\n",
      "Average recall: 1.0\n",
      "Average MAP: 0.35022893089455776\n",
      "\n",
      "Performance Table:\n",
      "Topic      | BM25      \n",
      "----------------------\n",
      "R101       | 0.5744047619047619\n",
      "R102       | 0.6363322454881951\n",
      "R103       | 0.22607885048707604\n",
      "R104       | 0.4822634740278421\n",
      "R105       | 0.4478223326042083\n",
      "R106       | 0.43599791013584116\n",
      "R107       | 0.53125   \n",
      "R108       | 0.08409961685823757\n",
      "R109       | 0.3985829218274427\n",
      "R110       | 0.26757964257964256\n",
      "R111       | 0.11858076563958915\n",
      "R112       | 0.4932178932178932\n",
      "R113       | 0.34900356664664495\n",
      "R114       | 0.7588235294117647\n",
      "R115       | 0.18204365079365079\n",
      "R116       | 0.39018475400608155\n",
      "R117       | 0.31666666666666665\n",
      "R118       | 0.4666666666666666\n",
      "R119       | 0.15200320512820512\n",
      "R120       | 0.24999157396320518\n",
      "R121       | 0.3125488389708701\n",
      "R122       | 0.5245554371668922\n",
      "R123       | 0.14183006535947715\n",
      "R124       | 0.14701930014430012\n",
      "R125       | 0.6080335639634574\n",
      "R126       | 0.871187113502701\n",
      "R127       | 0.3933333333333333\n",
      "R128       | 0.14345238095238094\n",
      "R129       | 0.23149351723154854\n",
      "R130       | 0.8055555555555555\n",
      "R131       | 0.21388888888888888\n",
      "R132       | 0.5611560008618832\n",
      "R133       | 0.22824508824508824\n",
      "R134       | 0.242005772005772\n",
      "R135       | 0.45176637481949194\n",
      "R136       | 0.21572420634920636\n",
      "R137       | 0.3833333333333333\n",
      "R138       | 0.08467770645300712\n",
      "R139       | 0.2686868686868687\n",
      "R140       | 0.7456621215979505\n",
      "R141       | 0.3697963021092741\n",
      "R142       | 0.15625   \n",
      "R143       | 0.10972222222222223\n",
      "R144       | 0.1530862815345574\n",
      "R145       | 0.3502081913846619\n",
      "R146       | 0.3079157719709639\n",
      "R147       | 0.15392209234121\n",
      "R148       | 0.2998325543446157\n",
      "R149       | 0.21543917435221785\n",
      "R150       | 0.25952445899254406\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_benchmark(benchmark_file):\n",
    "    with open(benchmark_file) as benFile:\n",
    "        file_ = benFile.readlines()\n",
    "    ben = {}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]] = float(lineList[2])\n",
    "    return ben\n",
    "\n",
    "def load_ranked_results(results_file):\n",
    "    rank = {}\n",
    "    with open(results_file) as f:\n",
    "        i = 1\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lineList = line.split()\n",
    "            # Strip '.xml' extension from document IDs\n",
    "            doc_id = lineList[0].replace('.xml', '')\n",
    "            rank[str(i)] = doc_id\n",
    "            i += 1\n",
    "    return rank\n",
    "\n",
    "\n",
    "def evaluate_query(ben, rank1):\n",
    "    ri = 0\n",
    "    map1 = 0.0\n",
    "    R = len([id for (id, v) in ben.items() if v > 0])\n",
    "    if R == 0:\n",
    "        return 0, 0, 0  # No relevant documents in the benchmark\n",
    "    for (n, doc_id) in sorted(rank1.items(), key=lambda x: int(x[0])):\n",
    "        if doc_id in ben and ben[doc_id] > 0:\n",
    "            ri += 1\n",
    "            pi = float(ri) / float(int(n))\n",
    "            map1 += pi\n",
    "    if ri > 0:\n",
    "        map1 = map1 / float(ri)\n",
    "    else:\n",
    "        map1 = 0\n",
    "    recall = float(ri) / float(R) if R > 0 else 0\n",
    "    return ri, recall, map1\n",
    "\n",
    "def print_performance_table(performance_data):\n",
    "    print(\"\\nPerformance Table:\")\n",
    "    print(f\"{'Topic':<10} | {'BM25':<10}\")\n",
    "    print(\"-\" * 22)\n",
    "    for topic, map_value in performance_data:\n",
    "        print(f\"{topic:<10} | {map_value:<10}\")\n",
    "\n",
    "def main():\n",
    "    query_range = range(101, 151)  # Adjust range as needed\n",
    "    benchmark_folder = \"EvaluationBenchmark\"\n",
    "    results_folder = \"My Code/Outputs-Task1-New\"\n",
    "    \n",
    "    total_relevant_items = 0\n",
    "    total_recall = 0\n",
    "    total_map = 0\n",
    "    total_queries = 0\n",
    "\n",
    "    performance_data = []\n",
    "\n",
    "    for query_id in query_range:\n",
    "        benchmark_file = os.path.join(benchmark_folder, f\"Dataset{query_id}.txt\")\n",
    "        results_file = os.path.join(results_folder, f\"BM25_R{query_id}Ranking.dat\")\n",
    "        \n",
    "        if os.path.exists(benchmark_file) and os.path.exists(results_file):\n",
    "            ben = load_benchmark(benchmark_file)\n",
    "            rank1 = load_ranked_results(results_file)\n",
    "            ri, recall, map1 = evaluate_query(ben, rank1)\n",
    "            total_relevant_items += ri\n",
    "            total_recall += recall\n",
    "            total_map += map1\n",
    "            total_queries += 1\n",
    "            print(f\"For Query {query_id}: relevant documents (ri)={ri}, recall={recall}, MAP={map1}\")\n",
    "            performance_data.append((f\"R{query_id}\", map1))\n",
    "        else:\n",
    "            print(f\"Files for query {query_id} do not exist.\")\n",
    "    \n",
    "    if total_queries > 0:\n",
    "        avg_recall = total_recall / total_queries\n",
    "        avg_map = total_map / total_queries\n",
    "    else:\n",
    "        avg_recall = 0\n",
    "        avg_map = 0\n",
    "\n",
    "    print(f\"\\nOverall results for {total_queries} queries:\")\n",
    "    print(f\"Total relevant items found: {total_relevant_items}\")\n",
    "    print(f\"Average recall: {avg_recall}\")\n",
    "    print(f\"Average MAP: {avg_map}\")\n",
    "\n",
    "    # Call the performance table function\n",
    "    print_performance_table(performance_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fc125-3bf4-4a72-8cd6-e048181c5c9e",
   "metadata": {},
   "source": [
    "### Now Testing code for evaluating model for all the queries benchmark with JM_LM model's results for those querries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c73a899-51fc-4e81-adea-c3af4c94c6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic      BM25     JM_LM\n",
      "0   R101  0.000000  0.000000\n",
      "1   R102  0.000000  0.000000\n",
      "2   R103  0.000000  0.000000\n",
      "3   R104  0.000000  0.000000\n",
      "4   R105  0.000000  0.000000\n",
      "5   R106  0.000000  0.000000\n",
      "6   R107  0.000000  0.000000\n",
      "7   R108  0.000000  0.000000\n",
      "8   R109  0.000000  0.000000\n",
      "9   R110  0.000000  0.000000\n",
      "10  R111  0.000000  0.000000\n",
      "11  R112  0.000000  0.000000\n",
      "12  R113  0.000000  0.000000\n",
      "13  R114  0.000000  0.000000\n",
      "14  R115  0.000000  0.000000\n",
      "15  R116  0.000000  0.000000\n",
      "16  R117  0.000000  0.000000\n",
      "17  R118  0.000000  0.000000\n",
      "18  R119  0.000000  0.000000\n",
      "19  R120  0.000000  0.000000\n",
      "20  R121  0.000000  0.000000\n",
      "21  R122  0.000000  0.000000\n",
      "22  R123  0.000000  0.000000\n",
      "23  R124  0.000000  0.000000\n",
      "24  R125  0.000000  0.000000\n",
      "25  R126  0.000000  0.000000\n",
      "26  R127  0.000000  0.000000\n",
      "27  R128  0.000000  0.000000\n",
      "28  R129  0.000000  0.000000\n",
      "29  R130  0.000000  0.000000\n",
      "30  R131  0.000000  0.000000\n",
      "31  R132  0.000000  0.000000\n",
      "32  R133  0.000000  0.000000\n",
      "33  R134  0.000000  0.000000\n",
      "34  R135  0.000000  0.000000\n",
      "35  R136  0.000000  0.000000\n",
      "36  R137  0.000000  0.000000\n",
      "37  R138  0.000000  0.000000\n",
      "38  R139  0.000000  0.000000\n",
      "39  R140  0.000000  0.000000\n",
      "40  R141  0.000000  0.000000\n",
      "41  R142  0.000000  0.000000\n",
      "42  R143  0.000000  0.000000\n",
      "43  R144  0.000000  0.000000\n",
      "44  R145  0.000000  0.000000\n",
      "45  R146  0.000000  0.000000\n",
      "46  R147  0.000000  0.000000\n",
      "47  R148  0.000000  0.000000\n",
      "48  R149  0.000000  0.000000\n",
      "49  R150  0.259524  0.306548\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_benchmark(benchmark_file):\n",
    "    with open(benchmark_file) as benFile:\n",
    "        file_ = benFile.readlines()\n",
    "    ben = {}\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "        lineList = line.split()\n",
    "        ben[lineList[1]] = float(lineList[2])\n",
    "    return ben\n",
    "\n",
    "def load_ranked_results(results_file):\n",
    "    rank = {}\n",
    "    with open(results_file) as f:\n",
    "        i = 1\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lineList = line.split()\n",
    "            # Strip '.xml' extension from document IDs\n",
    "            doc_id = lineList[0].replace('.xml', '')\n",
    "            rank[str(i)] = doc_id\n",
    "            i += 1\n",
    "    return rank\n",
    "\n",
    "\n",
    "def evaluate_query(ben, rank1):\n",
    "    ri = 0\n",
    "    map1 = 0.0\n",
    "    R = len([id for (id, v) in ben.items() if v > 0])\n",
    "    if R == 0:\n",
    "        return 0, 0, 0  # No relevant documents in the benchmark\n",
    "    for (n, doc_id) in sorted(rank1.items(), key=lambda x: int(x[0])):\n",
    "        if doc_id in ben and ben[doc_id] > 0:\n",
    "            ri += 1\n",
    "            pi = float(ri) / float(int(n))\n",
    "            map1 += pi\n",
    "    if ri > 0:\n",
    "        map1 = map1 / float(ri)\n",
    "    else:\n",
    "        map1 = 0\n",
    "    recall = float(ri) / float(R) if R > 0 else 0\n",
    "    return ri, recall, map1\n",
    "\n",
    "def create_performance_table(performance_data, models):\n",
    "    df_data = {'Topic': []}\n",
    "    for model in models:\n",
    "        df_data[model] = []\n",
    "\n",
    "    for topic, scores in performance_data.items():\n",
    "        df_data['Topic'].append(topic)\n",
    "        for model in models:\n",
    "            df_data[model].append(scores.get(model, \"N/A\"))\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    query_range = range(101, 151)  # Adjust range as needed\n",
    "    benchmark_folder = \"EvaluationBenchmark\"\n",
    "    models = {\n",
    "        \"BM25\": \"My Code/RankingOutputs/BM25_R{query_id}Ranking.dat\",\n",
    "        \"JM_LM\": \"My Code/RankingOutputs/JM_R{query_id}Ranking.dat\"\n",
    "    }\n",
    "    \n",
    "    performance_data = {f\"R{query_id}\": {} for query_id in query_range}\n",
    "\n",
    "    for model_name, results_pattern in models.items():\n",
    "        for query_id in query_range:\n",
    "            benchmark_file = os.path.join(benchmark_folder, f\"Dataset{query_id}.txt\")\n",
    "            results_file = results_pattern.format(query_id=query_id)\n",
    "            \n",
    "            if os.path.exists(benchmark_file) and os.path.exists(results_file):\n",
    "                ben = load_benchmark(benchmark_file)\n",
    "                rank1 = load_ranked_results(results_file)\n",
    "                ri, recall, map1 = evaluate_query(ben, rank1)\n",
    "                #print(f\"For Query {query_id} ({model_name}): relevant documents (ri)={ri}, recall={recall}, MAP={map1}\")\n",
    "                performance_data[f\"R{query_id}\"][model_name] = map1\n",
    "            else:\n",
    "                #print(f\"Files for query {query_id} ({model_name}) do not exist.\")\n",
    "                performance_data[f\"R{query_id}\"][model_name] = \"N/A\"\n",
    "\n",
    "    # Create DataFrame\n",
    "    performance_df = create_performance_table(performance_data, models.keys())\n",
    "    print(performance_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc61406-7b36-464e-a4d3-a0dc4bc105f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb801f0a-091a-46b2-9a8c-396dabf489fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
